---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>

I am a third-year undergraduate student majoring in Information and Computing Science at the School of Mathematics and Statistics, Wuhan University of Technology. My research interests primarily focus on **spatio-temporal sequence modeling**, **inference acceleration for large-scale models**, and **efficient attention mechanisms**.

In terms of research, I joined a time series research group in the second semester of my sophomore year, where I have been working on the design of efficient spatiotemporal forecasting architectures. During this period, I proposed LOSTFormer as the first author, a linear spatio-temporal Transformer with learnable orthogonal rotation attention, which significantly enhances model representational capacity while preserving theoretical consistency and linear computational complexity. The corresponding paper is currently under review.

At present, my research focus has expanded to applications of Learning to Hash, and I aspire to pursue further research in **LLM Long Context Modeling**, **Time Series Forecasting** and **ML systems (MLSys)**. In terms of engineering and competitive programming, I have received multiple awards in ICPC, CCPC, CCF-CSP, and other national-level programming competitions, demonstrating strong proficiency in algorithm design and system-level implementation.

I am passionate about advancing my understanding of Machine Learning and aspire to make meaningful contributions to the field in the
future. Please feel free to contact me if you‚Äôd like to discuss related research areas.üòÑ

<br>

# üî¨ Research Skills

- üí™ **Strongly self-motivated and passionate about research:** My experience includes systematic replication and improvement of deep learning models, as well as practical deployment and fine-tuning of large language models, bridging theoretical understanding with empirical validation.


- üíª **Excellent algorithm design and code implementation capabilities:** I won **bronze medals** at the ICPC 2024 Regionals
  and currently have a Codeforces Rating of [<span style="color: blue;">**1777**</span>](https://codeforces.com/profile/TheEndd). In addition, I have fully implemented the entire workflow for small-scale language models, **from pre-training to SFT**, enabling them to conduct conversations of a certain quality. Further details can be found in the Projection section of this paper.


- üìê **Theoretical derivation capability:** In the course of exploring efficient attention mechanisms, I undertook theoretical analysis and derivation of certain lemmas and theorems within the **Performer** framework.. To better align with my subsequent research directions, I also carried out theoretical derivations of several hashing methods, including **Spectral Hashing**, **Locality-Sensitive Hashing (LSH)**, and **Asymmetric Deep Supervised Hashing (ADSH)**.

<br>

# üìÖ News

- [2026.01] üéâOur paper [LOSTFormer](https://github.com/QAQYYC/LOSTFormer) has been accepted by DASFAA 2026!

- [2025.11] We won a bronze medal at the ICPC Wuhan Regionals.

- [2025.10] We won a bronze medal at the ICPC Chengdu Regionals.

- [2025.03] I started working on time series forecasting architecture under the guidance of
  Prof. [Liang Xie](http://ssci.whut.edu.cn/szdw/zrjs/202309/t20230921_939468.shtml).

- [2024.12] We won a bronze medal at the ICPC Hong Kong Regionals.

- [2024.11] We won a bronze medal at the CCPC Zhengzhou Regionals.

<br>

# ‚úçÔ∏è Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Accepted by DASFAA 2026</div><img src='images/paper_main/LOSTFormer.png' alt="sym" width="80%"></div></div>
<div class='paper-box-text' markdown="1">

**[1] LOSTFormer: Linear Orthogonal Spatio-Temporal  Transformer with Learnable Rotation**

**Yechen Yu**, Liang Xie, Jiankai Zheng, Peilin Tan

**Accepted by DASFAA 2026**

**Short Summary:** LOSTFormer is a linear spatio-temporal Transformer that uses learnable orthogonal rotation attention to efficiently model full spatio-temporal dependencies, achieving state-of-the-art forecasting accuracy with linear complexity.

<a href="https://github.com/QAQYYC/LOSTFormer"><img src="https://img.shields.io/github/stars/QAQYYC/LOSTFormer.svg" alt="Star Count"></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Submitted to ICML 2026</div><img src='images/paper_main/coming_soon.png' alt="sym" width="80%"></div></div>
<div class='paper-box-text' markdown="1">

**[2] Anoymous Paper**

**Second author**

**Submitted to ICML 2026**

[//]: # (**Short Summary:** LOSTFormer is a linear spatio-temporal Transformer that uses learnable orthogonal rotation attention to efficiently model full spatio-temporal dependencies, achieving state-of-the-art forecasting accuracy with linear complexity.)



</div>
</div>


<br>

# üèÜ Honors & Awards

## Programming Competitions

- **ICPC\CCPC International Collegiate Programming Contest** \
ü•â Regional Bronze Medalist (Hong Kong, Chengdu, Wuhan, Zhengzhou) \
Demonstrated strong problem-solving skills in algorithms and data structures, with a particular focus on graph theory and advanced data structures.


- **Group Programming Ladder Tournament (GPLT)** \
ü•à Individual Second Prize \
ü•à Team Second Prize (Ranked 1st within the team)


- **CCF Certified Software Professional (CSP)** \
üîπ Score: 300 \
Ranked in the Top 3% nationwide (40th CCF CSP examination).


- **RAICOM Robotics Development Competition 2025** \
ü•á National Final ‚Äì Programming Skills Track \
Achieved **full score**, demonstrating excellence in algorithmic implementation and engineering robustness.


- **Codeforces** \
üîπ Max Rating: [<span style="color: blue;">**1777**</span>](https://codeforces.com/profile/TheEndd) \
Specialized in graph algorithms, data structures, and competitive programming optimization. \
üëâ Contest template: [GitHub Repository](https://github.com/QAQYYC/ACM-Template)

## Honors & Scholarships
- **National Scholarship (China)** √ó1
- **Merit Student of Wuhan University of Technology** √ó1

<br>

# üíª Projections

## MiniMind (LLM Built from Scratch)

- **Model Architecture:** Developed a **LLaMA-like** language model from scratch using **PyTorch**, implementing core architectural components including **RoPE** (Rotary Positional Embeddings), **RMSNorm**, and **SwiGLU** activation functions.

- **Training Pipeline:** Engineered an end-to-end training pipeline encompassing both **Pretraining** on raw text and **Supervised Fine-Tuning (Full SFT)** for instruction following.

- **Inference Optimization:** Deployed a text generation script featuring **KV Cache** optimization for reduced latency and integrated **Top-p/Temperature** sampling for diverse, streaming responses.









